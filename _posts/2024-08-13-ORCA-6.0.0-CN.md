---
layout: post
title: ORCA 6.0.0安装与并行测试
subtitle: 仅供参考
author: ShuCi Lee
categories: ORCA
banner:
  image: http://free.yunpng.top/tuf/2024/08/13/66baf0efb83d9.jpg
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
tags: ORCA
sidebar: []
---

### 基本镜像信息

ubuntu:22.04-py3.10-cuda12.1

### 安装流程

下载[ORCA](https://orcaforum.kofo.mpg.de/)，我选择的是动态库版本：orca_6_0_0_linux_x86-64_shared_openmpi416.tar.xz

下载对应的[Open MPI](https://www.open-mpi.org/)，此例为4.1.6：openmpi-4.1.6.tar.bz2

安装sudo：
```
apt install sudo
```
安装gcc和gfortran：
```
sudo apt update
sudo apt install gcc
sudo apt install gfortran
```
新建一个文件夹，并解压、编译、安装Open MPI:
```
mkdir openmpi4.6.1
cd openmpi4.6.1 #假定你已经将openmpi-4.1.6.tar.bz2复制到这里
tar -xjvf openmpi-4.1.6.tar.bz2
./configure --prefix=/openmpi4.1.6 --disable-builtin-atomics
make all install #用-j本人出现了报错
```
将路径写入bashrc：
```
vi ~/.bashrc
export PATH=$PATH:/openmpi4.1.6/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/openmpi4.1.6/lib
```
编辑hostfile，注意如果你编译Open MPI的机子与实际计算的机子核心数不同，请在测试可以并行计算后自行修改 /openmpi4.1.6/etc/目录下的openmpi-default-hostfile
```
echo "localhost slots=$(cat /proc/cpuinfo | grep 'processor' | sort -u | wc -l)" >> /openmpi4.1.6/etc/openmpi-default-hostfile
```
解压下载的ORCA包，并写入bashrc
```
mkdir orca600
cd orca600 #假定你已经将orca_6_0_0_linux_x86-64_shared_openmpi416.tar.xz复制到这里
tar -xf orca_6_0_0_linux_x86-64_shared_openmpi416.tar.xz
vi ~/.bashrc
export PATH=$PATH:/orca600/orca_6_0_0_shared_openmpi416
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/orca600/orca_6_0_0_shared_openmpi416
alias orca='orca600/orca_6_0_0_shared_openmpi416/orca'
```
测试能否正常并行计算，在该教程中我使用的是4核心的机器
```
vi test.inp
```
复制粘贴以下内容
```
! BLYP def2-SVP noautostart miniprint pal4
* xyz 0 1
 C                 -2.30500585    0.80908031    0.00000000
 C                 -0.90984585    0.80908031    0.00000000
 C                 -0.21230785    2.01683131    0.00000000
 C                 -0.90996185    3.22534031   -0.00119900
 C                 -2.30478685    3.22526231   -0.00167800
 C                 -3.00238785    2.01705631   -0.00068200
 H                 -2.85476485   -0.14323669    0.00045000
 H                 -0.36033785   -0.14343269    0.00131500
 H                  0.88737215    2.01691131    0.00063400
 H                 -0.35976185    4.17748331   -0.00125800
 H                 -2.85490885    4.17754331   -0.00263100
 H                 -4.10199185    2.01723931   -0.00086200
*
```
保存并退出后使用以下命令进行测试：
```
orca test.inp > test.out
```
命令行输入top命令，可以看到有四个CPU占用较大的ORCA任务即为并行成功

自己常用的作业提交命令
```
/orca600/orca_6_0_0_shared_openmpi416/orca <input_file> > <input_file>.out
```

